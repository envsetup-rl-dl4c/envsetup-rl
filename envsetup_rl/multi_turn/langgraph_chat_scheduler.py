import asyncio
import heapq
import logging
import os
import time
import uuid
from typing import List

import hydra
from langchain_core.messages.utils import convert_to_openai_messages
from langchain_openai import ChatOpenAI
from omegaconf import DictConfig

from envsetup_rl.multi_turn.postprocess_multi_turn_output import postprocess
from verl.protocol import DataProto
from verl.utils.fs import copy_to_local
from verl.utils.tokenizer import hf_tokenizer
from verl.workers.rollout.async_server import ChatCompletionScheduler

logger = logging.getLogger(__file__)
logger.setLevel(os.getenv("VERL_LOGGING_LEVEL", "WARN"))


class LangGraphChatCompletionScheduler(ChatCompletionScheduler):
    """A scheduler for handling chat completions using LangGraph.

    This class extends ChatCompletionScheduler to provide async rollouts
    using LangGraph.
    """

    def __init__(
        self,
        config: DictConfig,
        model_path: str,
        server_addresses: List[str],
        max_cache_size: int = 10000,
    ):
        super().__init__(config, model_path, server_addresses, max_cache_size)
        langgraph_config = config.langgraph
        self.graph_partial = hydra.utils.instantiate(langgraph_config.graph, _partial_=True)
        self.chat_template_kwargs: dict = hydra.utils.instantiate(
            langgraph_config.get("chat_template_kwargs", {}),
            _convert_="all",  # important for tokenizer.apply_chat_template to work
        )
        self.graph_config = langgraph_config.get("graph_config", {})

        local_path = copy_to_local(model_path)
        self.tokenizer = hf_tokenizer(local_path, trust_remote_code=True)

    def assign_address(self):
        address = self.weighted_addresses[0][1]
        self.weighted_addresses[0][0] += 1  # type: ignore
        heapq.heapreplace(self.weighted_addresses, self.weighted_addresses[0])
        return address

    async def generate_sequences(self, batch: DataProto, **sampling_params) -> DataProto:  # type: ignore
        t_start = time.time()
        kwargs = dict(
            n=self.config.n,
            max_completion_tokens=self.config.response_length,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            extra_body={
                "chat_template_kwargs": {
                    "enable_thinking": self.config.get("enable_thinking", None),
                },
                # these are from openai.client.chat.completions.create arguments in ChatCompletionScheduler code
                "include_stop_str_in_output": True,
                "stop": ["</answer>"],
            },
        )

        do_sample = batch.meta_info.get("do_sample", True)
        is_validate = batch.meta_info.get("validate", False)
        if not do_sample or is_validate:
            kwargs["n"] = 1
            kwargs["temperature"] = 0

        kwargs.update(sampling_params)

        logger.info(f"generate_sequences sampling params: {kwargs}")

        tasks = []

        batch_tool_kwargs = batch.non_tensor_batch.get("tools_kwargs", None)

        for i, conversation in enumerate(batch.non_tensor_batch["raw_prompt"]):
            graph_input = dict(messages=list(conversation))
            # Pass tool kwargs if available
            if batch_tool_kwargs is not None:
                graph_input["tools_kwargs"] = batch_tool_kwargs[i]
            # Invoke LangGraph graph (n times)
            for _ in range(kwargs["n"]):
                # Get a OpenAI server address for the rollout
                address = self.assign_address()
                # Initialize LangChain LLM
                llm_kwargs = kwargs.copy()
                llm_kwargs["n"] = 1  # n samples are generated by invoking the graph n times, we need to set n=1 here
                llm = ChatOpenAI(base_url=f"http://{address}/v1", api_key="token-abc123", model=self.model_name, **llm_kwargs)  # type: ignore
                # Initialize LangGraph graph with the rollout LLM
                if "dummy" in self.config.langgraph.graph.get("_target_", ""):
                    graph = self.graph_partial(model=llm, tokenizer=self.tokenizer)
                else:
                    graph = self.graph_partial(model=llm)
                # Add thread_id to the graph config
                graph_config = self.graph_config.copy()
                if "configurable" not in graph_config:
                    graph_config["configurable"] = {}
                graph_config["configurable"]["thread_id"] = str(uuid.uuid4())
                # Submit a task to the pool
                tasks.append(
                    asyncio.create_task(
                        graph.ainvoke(
                            graph_input.copy(),
                            config=graph_config,
                        )
                    )
                )

        logger.info(f"submitted {len(batch.non_tensor_batch['raw_prompt'])} datapoints to LangGraph with n={kwargs['n']}")
        completed_messages = await asyncio.gather(*tasks, return_exceptions=True)
        logger.info(f"generated {len(completed_messages)} datapoints from LangGraph")

        batch_conversations = []
        for i, raw_prompt in enumerate(batch.non_tensor_batch["raw_prompt"]):
            conversations = completed_messages[i * kwargs["n"] : (i + 1) * kwargs["n"]]
            fixed_conversations = []
            for conversation in conversations:
                if isinstance(conversation, Exception):
                    logger.error(f"Failed to generate sequences for datapoint: {raw_prompt}. Using messages without assistant responses as fallback.", exc_info=conversation)
                    messages = raw_prompt
                else:
                    messages = conversation["messages"]
                fixed_conversations.append(convert_to_openai_messages(messages))
            batch_conversations.append(fixed_conversations)
        # _postprocess assumes n>=1
        output_batch = postprocess(model_name=self.model_name, tokenizer=self.tokenizer, batch=batch, batch_conversations=batch_conversations, n=kwargs["n"])
        output_batch.meta_info["timing"] = {"generate_sequences": time.time() - t_start}
        print("[LangGraphChatCompletionScheduler] generate_sequences done")
        return output_batch
